{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "989c4b10",
   "metadata": {},
   "source": [
    "# LangChain: Agents\n",
    "\n",
    "Some applications require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user’s input. In these types of chains, there is an agent which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.\n",
    "\n",
    "> You can think of a large language model as a reasoning engine, in which you can give it chunks of text or other sources of information. Then, the LLM can use that information (in addition to its background knowledge that's learned off the internet) to answer questions or reason through content or decide even what to do next. And that's what LangChain's Agents framework helps you to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561f7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # add .env to .gitignore\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1be24758",
   "metadata": {},
   "source": [
    "## Built-in LangChain tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4cb7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install -U wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddad01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29bd3f25",
   "metadata": {},
   "source": [
    "Setting the temperature equal to zero is important because we're going to be using the LLM as the reasoning engine of an agent, where it's connecting to other sources of data and computation.\n",
    "\n",
    "And so we want this reasoning engine to be as good and as precise as possible and so we're going to set it to zero to get rid of any randomness that might arise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4e69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46687047",
   "metadata": {},
   "source": [
    "Next, we're going to load the LLM Math tool and the Wikipedia tool.\n",
    "\n",
    "\n",
    "The LLM Math tool is actually a chain itself, which uses a language model in conjunction with a calculator to do math problems. The Wikipedia tool is an API that connects to Wikipedia, allowing you to run search queries against Wikipedia and get back results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a1c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fda6c17a",
   "metadata": {},
   "source": [
    "We're going to initialize the agent with the following parameters:\n",
    "* tools \n",
    "* the language model \n",
    "* agent type: Here, we're going to use chat zero-shot React description. The important things to note here are first, **CHAT**, This is an agent that has been optimized to work with chat models. And second, **REACT**, This is a prompting technique designed to get the best reasoning performance out of language models.\n",
    "* ``handleParsingErrors`` equals true. This is useful when the language model might output something that is not able to be parsed into an action and action input, which is the desired output. When this happens, we'll actually pass the misformatted text back to the language model and ask it to correct itself.\n",
    "* ``verbose`` equals true. Prints out a bunch of steps that makes it really clear to us in the Jupyter Notebook what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee5ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e2a10f0",
   "metadata": {},
   "source": [
    "### Math tool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90972c7c",
   "metadata": {},
   "source": [
    "Let's ask the agent a simple math question to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a5c972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can use the calculator tool to find the answer to this question.\n",
      "\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"25% of 300\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe answer is 75.0.\n",
      "Final Answer: 75.0\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the 25% of 300?', 'output': '75.0'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"What is the 25% of 300?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d130796d",
   "metadata": {},
   "source": [
    "We can see above that when it enters the ``AgentExecutor chain``, that it first thinks about what it needs to do. \n",
    "\n",
    "So it has a ``Thought``. It then has an ``Action``, and this action is actually a JSON blob corresponding to two things, an ``action`` and an ``action input``: \n",
    "\n",
    "* ``action`` corresponds to the tool to use. So here it says calculator. \n",
    "* ``action input`` is the input to that tool. Here it's a string of 300 times 0.25. \n",
    "\n",
    "Next, we can see that there's ``Observation`` with answer in a **separate color**. This observation answer equals 75.0 is actually coming from the **calculator** tool itself.\n",
    "\n",
    "Next, we go back to the LLM when the text turns to green (``Thought``). We have the answer to the question.\n",
    "\n",
    "Final answer, 75.0, and that's the output that we get.\n",
    "\n",
    "> NOTE: Different colors are used in the output to denote different tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c755b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-28 09:44.33.094179 [info     ] Trace: http://127.0.0.1:8935/traces/01H6DQ9NT55Z7FVMR98MK9SAV2\n",
      "\n",
      "\n",
      "> Entering new AgentExecutor chain...\n",
      "I can use the calculator tool to find the answer to this question.\n",
      "\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"25% of 300\"\n",
      "}\n",
      "```2023-07-28 09:44.35.338825 [info     ] Starting server, set OUGHT_ICE_AUTO_SERVER=0 to disable.\n",
      "\n",
      "Observation: Answer: 75.0\n",
      "Thought:2023-07-28 09:44.36.889774 [info     ] Server started! Run `python -m ice.server stop` to stop it.\n",
      "2023-07-28 09:44.36.891686 [info     ] Opening trace in browser, set OUGHT_ICE_AUTO_BROWSER=0 to disable.\n",
      "The answer is 75.0.\n",
      "Final Answer: 75.0\n",
      "\n",
      "> Finished chain.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'What is the 25% of 300?'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'output'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'75.0'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'input'\u001b[0m: \u001b[32m'What is the 25% of 300?'\u001b[0m, \u001b[32m'output'\u001b[0m: \u001b[32m'75.0'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use visualization tool \n",
    "from langchain_visualizer import visualize, visualize_embeddings\n",
    "\n",
    "async def run_chain():\n",
    "    return agent(\"What is the 25% of 300?\")\n",
    "\n",
    "visualize_embeddings()\n",
    "visualize(run_chain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c6681b9",
   "metadata": {},
   "source": [
    "### Wikipedia tool\n",
    "Next, we're going to go through an example using the Wikipedia API. Here, we are asking a question about Tom Mitchell, and we can look at the intermediate steps to see what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1cf336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should use Wikipedia to find the answer to this question.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Tom M. Mitchell\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Tom M. Mitchell\n",
      "Summary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past President of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.\n",
      "\n",
      "Page: Tom Mitchell (Australian footballer)\n",
      "Summary: Thomas Mitchell (born 31 May 1993) is a professional Australian rules footballer playing for the Collingwood Football Club in the Australian Football League (AFL). He previously played for the Sydney Swans from 2012 to 2016, and the Hawthorn Football Club between 2017 and 2022. Mitchell won the Brownlow Medal as the league's best and fairest player in 2018 and set the record for the most disposals in a VFL/AFL match, accruing 54 in a game against Collingwood during that season.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe book that Tom M. Mitchell wrote is called \"Machine Learning\".\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Machine Learning (book)\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Machine learning\n",
      "Summary: Machine learning (ML) is a field devoted to understanding and building methods that let machines \"learn\" – that is, methods that leverage data to improve computer performance on some set of tasks.Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.In its application across business problems, machine learning is also referred to as predictive analytics.\n",
      "\n",
      "Page: Timeline of machine learning\n",
      "Summary: This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events in machine learning are included.\n",
      "\n",
      "Page: Quantum machine learning\n",
      "Summary: Quantum machine learning is the integration of quantum algorithms within machine learning programs. The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data. Beyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments. Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa. Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mTom M. Mitchell wrote the book \"Machine Learning\".\n",
      "\n",
      "Final Answer: Machine Learning.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\"\n",
    "result = agent(question) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03c1e742",
   "metadata": {},
   "source": [
    "We can see once again that it thinks and it correctly realizes that it should use Wikipedia.\n",
    "\n",
    "It says ``action`` equal to Wikipedia and ``action input`` equal to Tom M. Mitchell.\n",
    "\n",
    "The ``Observation`` that comes back (in yellow this time,) is the Wikipedia summary result for the Tom M. Mitchell page.\n",
    "\n",
    "The ``Observation`` that comes back from Wikipedia is actually two results, two pages as there's two different Tom M. Mitchells (the computer scientist and an Australian footballer)\n",
    "\n",
    "We can see that the information needed to answer this question, namely the name of the book that he wrote (Machine Learning) is present in the summary of the first Tom M. Mitchell.\n",
    "\n",
    "We can see next that the agent tries to look up more information about this book.\n",
    "\n",
    "It looks up Machine Learning book in Wikipedia. This isn't strictly necessary, and it's an interesting example to show how agents aren't perfectly reliable yet.\n",
    "\n",
    "We can see that after this lookup, the agent recognizes that it has all the information it needs to answer, and responds with the correct answer, Machine Learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3fb7335f",
   "metadata": {},
   "source": [
    "## Python Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69345368",
   "metadata": {},
   "source": [
    "If you've seen things like Copilot or even ChatGPT with the code interpreter plugin enabled, one of the things they're doing is they're using the language model to write code, and then executing that code. We can do the same exact thing here. \n",
    "\n",
    "We're going to create a Python agent, and we're going to use the same LLM as before, and we're going to give it a tool, the Python REPL tool. A REPL is basically a way to interact with code. You can think of it as a Jupyter Notebook. The agent can execute code with this REPL. It will then run, and then we'll get back some results. \n",
    "\n",
    "Those results will be passed back into the agent, so it can decide what to do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc6ca899",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc6b62d4",
   "metadata": {},
   "source": [
    "The problem that we're going to have this agent solve is we're going to give it a list of names, and then ask it to sort them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7cd8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_list = [[\"Harrison\", \"Chase\"], \n",
    "                 [\"Lang\", \"Chain\"],\n",
    "                 [\"Dolly\", \"Too\"],\n",
    "                 [\"Elle\", \"Elem\"], \n",
    "                 [\"Geoff\",\"Fusion\"], \n",
    "                 [\"Trance\",\"Former\"],\n",
    "                 [\"Jen\",\"Ayai\"]\n",
    "                ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a2205b3",
   "metadata": {},
   "source": [
    "Importantly, we're asking it to print the output, so that it can actually see what the result is. These printed statements are what's going to be fed back into the language model later on, so it can reason about the output of the code that it just ran. Let's give this a try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32cd93dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\n",
      "Action: Python REPL\n",
      "Action Input:\n",
      "```\n",
      "customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\n",
      "sorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\n",
      "for customer in sorted_customers:\n",
      "    print(customer)\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Jen', 'Ayai']\n",
      "['Lang', 'Chain']\n",
      "['Harrison', 'Chase']\n",
      "['Elle', 'Elem']\n",
      "['Trance', 'Former']\n",
      "['Geoff', 'Fusion']\n",
      "['Dolly', 'Too']\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe customers are now sorted by last name and then first name. \n",
      "Final Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54346964",
   "metadata": {},
   "source": [
    "We can see that when we go into the ``AgentExecutor chain``, it first realizes that it can use the sorted function to list the customers. It's using a different agent type under the hood, which is why you can see that the ``action`` and ``action input`` is actually formatted slightly differently. \n",
    "\n",
    "Here, the ``action`` that it takes is to use the Python ``REPL``, and then the ``action input`` that you can see is code, where it first writes out customers equals this list, it then sorts the customers, and then it goes through this list and print it. \n",
    "\n",
    "You can see the agent thinks about what to do and realizes that it needs to write some code. The format that it's using of ``action`` and ``action input`` is actually slightly different than before. It's using a **different agent type** under the hood. For the ``action``, it's going to use the Python ``REPL``, and for the action input, it's going to have a bunch of code. \n",
    "\n",
    "If we look at what this code is doing, it's first creating a variable to list out these customer names. It's then sorting that and creating a new variable, and it's then iterating through that new variable and printing out each line, just like we asked it to. \n",
    "\n",
    "We can see that we get the ``Observation`` back, and this is a list of names, and then the agent realizes that it's done and it returns these names. We can see from the stuff that's printed out the high level of what's going on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe4fb7fb",
   "metadata": {},
   "source": [
    "#### View detailed outputs of the chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "890e10fc",
   "metadata": {},
   "source": [
    "Let's dig a little bit deeper and prints out all the levels of all the different chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eaec6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [8.01s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\\nAction: Python REPL\\nAction Input:\\n```\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nfor customer in sorted_customers:\\n    print(customer)\\n```\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"I can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\\nAction: Python REPL\\nAction Input:\\n```\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nfor customer in sorted_customers:\\n    print(customer)\\n```\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 327,\n",
      "      \"completion_tokens\": 144,\n",
      "      \"total_tokens\": 471\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [8.01s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\\nAction: Python REPL\\nAction Input:\\n```\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nfor customer in sorted_customers:\\n    print(customer)\\n```\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:Python REPL] Entering Tool run with input:\n",
      "\u001b[0m\"```\n",
      "customers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\n",
      "sorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\n",
      "for customer in sorted_customers:\n",
      "    print(customer)\n",
      "```\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:Python REPL] [0.0ms] Exiting Tool run with output:\n",
      "\u001b[0m\"['Jen', 'Ayai']\n",
      "['Lang', 'Chain']\n",
      "['Harrison', 'Chase']\n",
      "['Elle', 'Elem']\n",
      "['Trance', 'Former']\n",
      "['Geoff', 'Fusion']\n",
      "['Dolly', 'Too']\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\",\n",
      "  \"agent_scratchpad\": \"I can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\\nAction: Python REPL\\nAction Input:\\n```\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nfor customer in sorted_customers:\\n    print(customer)\\n```\\nObservation: ['Jen', 'Ayai']\\n['Lang', 'Chain']\\n['Harrison', 'Chase']\\n['Elle', 'Elem']\\n['Trance', 'Former']\\n['Geoff', 'Fusion']\\n['Dolly', 'Too']\\n\\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nThought:I can use the sorted() function to sort the list of customers by last name and then first name. I will need to provide a key function to sorted() that returns a tuple of the last name and first name in that order.\\nAction: Python REPL\\nAction Input:\\n```\\ncustomers = [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']]\\nsorted_customers = sorted(customers, key=lambda x: (x[1], x[0]))\\nfor customer in sorted_customers:\\n    print(customer)\\n```\\nObservation: ['Jen', 'Ayai']\\n['Lang', 'Chain']\\n['Harrison', 'Chase']\\n['Elle', 'Elem']\\n['Trance', 'Former']\\n['Geoff', 'Fusion']\\n['Dolly', 'Too']\\n\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] [5.36s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 526,\n",
      "      \"completion_tokens\": 67,\n",
      "      \"total_tokens\": 593\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] [5.36s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The customers have been sorted by last name and then first name.\\nFinal Answer: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [13.37s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"[['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.debug=True\n",
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\") \n",
    "langchain.debug=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56971a27",
   "metadata": {},
   "source": [
    "1. First, we start with the ``[chain/start] [1:chain:AgentExecutor]``. This is the top level agent runner, and we can see that we have here our input.\n",
    "\n",
    "2. From ``[chain/start] [1:chain:AgentExecutor > 2:chain:LLMChain]``, we call an ``LLM chain`` that the agent is using. At this point, it's only got the ``input``, ``agent scratchpad``. and some ``stop`` sequences to tell the language model when to stop doing its generations.\n",
    "\n",
    "> Remember: **LLM chain** is a combination of prompt and an LLM.\n",
    "\n",
    "3. At the next level `[llm/start] [1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI]`, we see the exact call to the language model. We can see the fully formatted ``prompt``, which includes instructions about what tools it has access to, as well as how to format its output.\n",
    "\n",
    "4. From ``[llm/end] [1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI]``, we can then see the exact output of the language model. We can see the ``text`` key where it has the thought and the action and the action input all in one string.\n",
    "\n",
    "5. `[chain/end] [1:chain:AgentExecutor > 2:chain:LLMChain]` It then wraps up the LLM chain as it exits through there.\n",
    "---\n",
    "6. `[tool/start] [1:chain:AgentExecutor > 4:tool:Python REPL]`: The next thing that it calls is a tool. Here, we can see the exact input to the tool (Python REPL) Then we can see the input, which is this code.\n",
    "\n",
    "7. `[tool/end] [1:chain:AgentExecutor > 4:tool:Python REPL]` We can then see the output of this tool, which is this printed out string, because we've specifically asked the Python REPL to print out what is going on.\n",
    "\n",
    "8. `[chain/start] [1:chain:AgentExecutor > 5:chain:LLMChain]` We can then see the next input to the LLM chain, which again, the LLM chain here is the agent. Here, if you look at the variables, the ``input`` is unchanged. It is the high-level objective that we're asking. But now there's some new values for ``agent scratchpad``, which is actually a combination of the previous generation plus the tool output We're passing ``agent scratchpad`` back in so that the language model can understand what happened previously and use that to reason about what to do next.\n",
    "\n",
    "The next few print statements are covering what happens as the language model realizes that it is basically finished with its job:\n",
    "\n",
    "9. ``[llm/start] [1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI]``: We can see here the fully formatted prompt to the language model, \n",
    "10. ``[llm/end] [1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI]``: the response where it realizes that it is done, and it says final answer, which here is the sequence that the agent uses to recognize that it's done with its job.\n",
    "\n",
    "11. We can then see it exiting the LLM chain `[chain/end] [1:chain:AgentExecutor > 5:chain:LLMChain]` and then exiting the agent executor `[chain/end] [1:chain:AgentExecutor]`.\n",
    "\n",
    "This should hopefully give you a pretty good idea of what's going on under the hood inside these agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f5aad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-28 09:49.57.696752 [info     ] Trace: http://127.0.0.1:8935/traces/01H6DQKJSZAZ3D6WY3GYBACZRV\n",
      "\n",
      "\n",
      "> Entering new AgentExecutor chain...\n",
      "2023-07-28 09:49.57.731538 [info     ] Opening trace in browser, set OUGHT_ICE_AUTO_BROWSER=0 to disable.\n",
      "I can use the `sorted()` function to sort the list of customers. I will need to provide a key function that specifies the sorting order based on last name and then first name.\n",
      "Action: Python_REPL\n",
      "Action Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \n",
      "Thought:The customers have been sorted by last name and then first name.\n",
      "Final Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\n",
      "\n",
      "> Finished chain.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Jen'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Ayai'</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Harrison'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Chase'</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Lang'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Chain'</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Elle'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Elem'</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Geoff'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Fusion'</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Trance'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Former'</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Dolly'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Too'</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'Jen'\u001b[0m, \u001b[32m'Ayai'\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[32m'Harrison'\u001b[0m, \u001b[32m'Chase'\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[32m'Lang'\u001b[0m, \u001b[32m'Chain'\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[32m'Elle'\u001b[0m, \u001b[32m'Elem'\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[32m'Geoff'\u001b[0m, \u001b[32m'Fusion'\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[32m'Trance'\u001b[0m, \u001b[32m'Former'\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[32m'Dolly'\u001b[0m, \u001b[32m'Too'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async def run_chain():\n",
    "    return agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\")\n",
    "\n",
    "visualize_embeddings()\n",
    "visualize(run_chain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69b66620",
   "metadata": {},
   "source": [
    "## Define your own tool"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07db7aa7",
   "metadata": {},
   "source": [
    "The big power of agents is that you can connect it to your own sources of information, your own APIs, your own data. That is, you can create a custom tool so that you can connect it to whatever you want.\n",
    "\n",
    "Let's make a tool that's going to tell us what the current date is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "625a80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install DateTime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9d31a0c",
   "metadata": {},
   "source": [
    "First, we're going to import this ``tool`` decorator. This can be applied to ANY function, and it turns it into a tool that LangChain can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "776bc581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "842c6ad8",
   "metadata": {},
   "source": [
    "Next, we're going to write a function called ``time``, which takes in any text string and returns today's date by calling dateTime.\n",
    "\n",
    "In addition to the name of the function, we're also going to write a really detailed doc string. That's because this is what the ``agent`` will use to know when it should call this tool and how it should call this tool.\n",
    "\n",
    "For example, here we say that the input should always be an empty string. That's because we don't use it. If we have more string requirements on what the input should be, for example, if we have a function that should always take in a search query or a SQL statement, you'll want to make sure to mention that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f58e5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def time(text: str) -> str:\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "026f233a",
   "metadata": {},
   "source": [
    "We're now going to create another ``agent``. This time we're adding the time tool to the list of existing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5134b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= initialize_agent(\n",
    "    tools + [time], \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47313945",
   "metadata": {},
   "source": [
    "And finally, let's call the ``agent`` and ask it what the date today is:\n",
    "\n",
    "> **Note**: The agent will sometimes come to the wrong conclusion (agents are a work in progress!). If it does, please try running it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c698a056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use the `time` tool to get today's date.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"time\",\n",
      "  \"action_input\": \"\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m2023-06-13\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have successfully retrieved today's date using the `time` tool.\n",
      "Final Answer: Today's date is 2023-06-13.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = agent(\"whats the date today?\") \n",
    "except: \n",
    "    print(\"exception on external access\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "354e9fa6",
   "metadata": {},
   "source": [
    " The `agent` recognizes that it needs to use the ``time`` tool, which it specifies above. It has the ``action input`` as an empty string. This is great. This is what we told it to do. Then, it returns with an ``Observation``."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
